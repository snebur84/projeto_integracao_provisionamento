# URL: https://github.com/snebur84/projeto_integracao_provisionamento/blob/main/.github/workflows/deploy-terraform-ec2.yml
# GitHub Actions workflow:
# - If an instance with the configured project/name tag already exists, skip Terraform and reuse it.
# - Otherwise run Terraform to create infra and then deploy.
# - After infra is ensured, upload the provision script to S3, ensure dynamic secrets in SSM and run the provision script via SSM.
#
# Required repository Secrets / Variables (create them in GitHub > Settings > Secrets and variables > Actions):
# - AWS_ACCESS_KEY_ID (secret)
# - AWS_SECRET_ACCESS_KEY (secret)
# - AWS_REGION (variable or secret)
# - DJANGO_SUPERUSER_USERNAME (secret)
# - DJANGO_SUPERUSER_EMAIL (secret)
# - DJANGO_SUPERUSER_PASSWORD (secret)
# - PROVISION_API_KEY (secret) [optional — if you want to control it via GH Secrets]
# - TF_VAR_public_key (optional secret) and other TF_VAR_* as required by infra/terraform
# - S3_BUCKET (optional): if you already have a staging bucket and want to reuse it when skipping Terraform
#
# Inputs:
# - environment: logical environment name (used as SSM prefix and tag filter)
#
# Notes:
# - The check looks for EC2 instances by tag "Project" equal to the environment name. Adjust the tag/filter if your infra uses a different tag.
# - If you prefer a different detection method (by Name tag, by tag value prefix or by explicit instance id), adjust the check_infra job's AWS CLI filters.
# - If Terraform creates a staging bucket, its output "staging_bucket" will be used. If Terraform is skipped and you provided S3_BUCKET secret, that bucket is used instead.
#
on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment (used as tag and SSM prefix)'
        required: false
        default: 'prod'

jobs:
  check_infra:
    name: Check existing infra (EC2)
    runs-on: ubuntu-latest
    outputs:
      found: ${{ steps.check.outputs.found }}
      instance_id: ${{ steps.check.outputs.instance_id }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Check for existing EC2 instance by Project tag
        id: check
        run: |
          set -euo pipefail
          ENV="${{ github.event.inputs.environment }}"
          # you can change the tag key if your infra uses a different one (e.g. "Name")
          TAG_KEY="Project"

          echo "Looking for instances with tag ${TAG_KEY}=${ENV} (state running|pending|stopped|stopping)..."
          # Query instances by tag and acceptable states
          INSTANCES=$(aws ec2 describe-instances \
            --filters "Name=tag:${TAG_KEY},Values=${ENV}" "Name=instance-state-name,Values=pending,running,stopping,stopped" \
            --query 'Reservations[].Instances[].InstanceId' --output text || true)

          if [ -z "$INSTANCES" ]; then
            echo "No instance found for ${TAG_KEY}=${ENV}"
            echo "found=false" >> "$GITHUB_OUTPUT"
            echo "instance_id=" >> "$GITHUB_OUTPUT"
          else
            # pick the first instance id
            FIRST=$(echo "$INSTANCES" | awk '{print $1}')
            echo "Found instance: $FIRST"
            echo "found=true" >> "$GITHUB_OUTPUT"
            echo "instance_id=$FIRST" >> "$GITHUB_OUTPUT"
          fi

  terraform:
    name: Terraform apply (create infra) — runs only if no infra found
    runs-on: ubuntu-latest
    needs: check_infra
    if: needs.check_infra.outputs.found == 'false'
    outputs:
      instance_id: ${{ steps.set_outputs.outputs.instance_id }}
      staging_bucket: ${{ steps.set_outputs.outputs.staging_bucket }}
    env:
      TF_IN_AUTOMATION: "true"
      TF_VAR_public_key: ${{ secrets.TF_VAR_public_key || '' }}
      TF_VAR_tfstate_bucket: ${{ secrets.TF_VAR_tfstate_bucket || '' }}
      TF_VAR_tfstate_lock_table: ${{ secrets.TF_VAR_tfstate_lock_table || '' }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: '1.5.0'

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Terraform init
        working-directory: infra/terraform
        run: terraform init -input=false

      - name: Terraform validate
        working-directory: infra/terraform
        run: terraform validate

      - name: Terraform plan
        working-directory: infra/terraform
        run: terraform plan -input=false -out=tfplan

      - name: Terraform apply
        working-directory: infra/terraform
        run: terraform apply -input=false -auto-approve tfplan

      - name: Export terraform outputs
        id: set_outputs
        working-directory: infra/terraform
        run: |
          terraform output -json > /tmp/tf_outputs.json
          cat /tmp/tf_outputs.json
          INSTANCE_ID=$(jq -r '.instance_id.value' /tmp/tf_outputs.json)
          STAGING_BUCKET=$(jq -r '.staging_bucket.value' /tmp/tf_outputs.json)
          echo "instance_id=$INSTANCE_ID" >> "$GITHUB_OUTPUT"
          echo "staging_bucket=$STAGING_BUCKET" >> "$GITHUB_OUTPUT"

  deploy:
    name: Upload + Deploy to EC2 (via SSM)
    runs-on: ubuntu-latest
    needs: [check_infra, terraform]
    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      ENVIRONMENT: ${{ github.event.inputs.environment }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Determine instance_id and staging bucket to use
        id: prep
        run: |
          set -euo pipefail
          # Determine INSTANCE_ID: prefer check_infra found instance, otherwise use terraform output
          CHECK_FOUND="${{ needs.check_infra.outputs.found }}"
          CHECK_INSTANCE="${{ needs.check_infra.outputs.instance_id }}"
          TF_INSTANCE="${{ needs.terraform.outputs.instance_id }}"
          # Determine STAGING_BUCKET: prefer terraform output if TF ran; else use provided S3_BUCKET secret if present
          TF_STAGING="${{ needs.terraform.outputs.staging_bucket }}"
          S3_BUCKET_OVERRIDE="${{ secrets.S3_BUCKET || '' }}"

          if [ "$CHECK_FOUND" = "true" ] && [ -n "$CHECK_INSTANCE" ]; then
            INSTANCE_ID="$CHECK_INSTANCE"
            echo "Using existing instance from check_infra: $INSTANCE_ID"
          elif [ -n "$TF_INSTANCE" ]; then
            INSTANCE_ID="$TF_INSTANCE"
            echo "Using instance from terraform output: $INSTANCE_ID"
          else
            echo "No instance id available from check_infra or terraform. Failing."
            exit 1
          fi

          if [ -n "$TF_STAGING" ] && [ "$TF_STAGING" != "null" ]; then
            STAGING_BUCKET="$TF_STAGING"
            echo "Using staging bucket from terraform: $STAGING_BUCKET"
          elif [ -n "$S3_BUCKET_OVERRIDE" ]; then
            STAGING_BUCKET="$S3_BUCKET_OVERRIDE"
            echo "Using staging bucket from secrets: $STAGING_BUCKET"
          else
            echo "No staging bucket available (terraform did not run and no S3_BUCKET secret configured). Failing."
            exit 1
          fi

          echo "instance_id=$INSTANCE_ID" >> "$GITHUB_OUTPUT"
          echo "staging_bucket=$STAGING_BUCKET" >> "$GITHUB_OUTPUT"

      - name: Upload provision script to staging bucket
        run: |
          STAGING_BUCKET="${{ steps.prep.outputs.staging_bucket }}"
          aws s3 cp scripts/provision_ubuntu_ec2.sh s3://$STAGING_BUCKET/provision_ubuntu_ec2.sh --acl private
          echo "Uploaded scripts/provision_ubuntu_ec2.sh to s3://$STAGING_BUCKET/"

      - name: Ensure dynamic secrets exist in SSM (SECRET_KEY, DB_PASSWORD, PROVISION_API_KEY, superuser)
        id: ensure_ssm
        env:
          ENV_PATH: "/provision/${{ github.event.inputs.environment }}"
        run: |
          set -euo pipefail
          check_and_put() {
            local name="$1"; local value="$2"
            if aws ssm get-parameter --name "$name" --with-decryption >/dev/null 2>&1; then
              echo "SSM parameter $name exists, skipping"
            else
              aws ssm put-parameter --name "$name" --value "$value" --type "SecureString" --overwrite
              echo "Created SSM parameter $name"
            fi
          }

          SECRET_KEY=$(python - <<'PY'
import secrets
print(secrets.token_urlsafe(50))
PY
)
          DB_PASS=$(openssl rand -base64 18)
          if [ -n "${{ secrets.PROVISION_API_KEY }}" ]; then
            PROVISION_KEY="${{ secrets.PROVISION_API_KEY }}"
          else
            PROVISION_KEY=$(uuidgen)
          fi

          SK="${ENV_PATH}/SECRET_KEY"
          DBP="${ENV_PATH}/DB_PASSWORD"
          PK="${ENV_PATH}/PROVISION_API_KEY"
          SUP_USER="${ENV_PATH}/DJANGO_SUPERUSER_USERNAME"
          SUP_EMAIL="${ENV_PATH}/DJANGO_SUPERUSER_EMAIL"
          SUP_PASS="${ENV_PATH}/DJANGO_SUPERUSER_PASSWORD"

          check_and_put "$SK" "$SECRET_KEY"
          check_and_put "$DBP" "$DB_PASS"
          check_and_put "$PK" "$PROVISION_KEY"

          if [ -n "${{ secrets.DJANGO_SUPERUSER_USERNAME }}" ]; then
            aws ssm put-parameter --name "$SUP_USER" --value "${{ secrets.DJANGO_SUPERUSER_USERNAME }}" --type "SecureString" --overwrite
          fi
          if [ -n "${{ secrets.DJANGO_SUPERUSER_EMAIL }}" ]; then
            aws ssm put-parameter --name "$SUP_EMAIL" --value "${{ secrets.DJANGO_SUPERUSER_EMAIL }}" --type "SecureString" --overwrite
          fi
          if [ -n "${{ secrets.DJANGO_SUPERUSER_PASSWORD }}" ]; then
            aws ssm put-parameter --name "$SUP_PASS" --value "${{ secrets.DJANGO_SUPERUSER_PASSWORD }}" --type "SecureString" --overwrite
          fi

      - name: Send SSM RunCommand to prepare .env and execute provision script
        id: send_ssm
        env:
          INSTANCE_ID: ${{ steps.prep.outputs.instance_id }}
          STAGING_BUCKET: ${{ steps.prep.outputs.staging_bucket }}
          ENV_PATH: "/provision/${{ github.event.inputs.environment }}"
        run: |
          set -euo pipefail
          INSTANCE_ID="${INSTANCE_ID}"
          STAGING_BUCKET="${STAGING_BUCKET}"
          ENV_PATH="${ENV_PATH}"

          REMOTE_SCRIPT=$(cat <<'EOF'
#!/bin/bash
set -euo pipefail
export PATH=/usr/local/bin:/usr/bin:/bin

get_ssm() {
  aws ssm get-parameter --name "$1" --with-decryption --query Parameter.Value --output text 2>/dev/null || echo ""
}

ENV_PATH="$ENV_PATH"
mkdir -p /home/provision
chown provision:provision /home/provision || true

SECRET_KEY=$(get_ssm "${ENV_PATH}/SECRET_KEY")
DB_PASSWORD=$(get_ssm "${ENV_PATH}/DB_PASSWORD")
PROVISION_API_KEY=$(get_ssm "${ENV_PATH}/PROVISION_API_KEY")
DJANGO_SUPERUSER_USERNAME=$(get_ssm "${ENV_PATH}/DJANGO_SUPERUSER_USERNAME")
DJANGO_SUPERUSER_EMAIL=$(get_ssm "${ENV_PATH}/DJANGO_SUPERUSER_EMAIL")
DJANGO_SUPERUSER_PASSWORD=$(get_ssm "${ENV_PATH}/DJANGO_SUPERUSER_PASSWORD")

cat > /tmp/provision.env <<EOL
SECRET_KEY='${SECRET_KEY}'
MYSQL_PASSWORD='${DB_PASSWORD}'
PROVISION_API_KEY='${PROVISION_API_KEY}'
DJANGO_SETTINGS_MODULE=provision.settings
ALLOWED_HOSTS=*
DJANGO_SUPERUSER_USERNAME='${DJANGO_SUPERUSER_USERNAME}'
DJANGO_SUPERUSER_EMAIL='${DJANGO_SUPERUSER_EMAIL}'
DJANGO_SUPERUSER_PASSWORD='${DJANGO_SUPERUSER_PASSWORD}'
EOL

mv /tmp/provision.env /home/provision/.env || cp /tmp/provision.env /home/provision/.env
chown provision:provision /home/provision/.env || true
chmod 600 /home/provision/.env || true

aws s3 cp "s3://${STAGING_BUCKET}/provision_ubuntu_ec2.sh" /tmp/provision_ubuntu_ec2.sh
chmod +x /tmp/provision_ubuntu_ec2.sh
/bin/bash /tmp/provision_ubuntu_ec2.sh
EOF
)

          CMD_ID=$(aws ssm send-command \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --comment "Deploy from GitHub Actions - prepare env and run provision script" \
            --parameters commands=["$REMOTE_SCRIPT"] \
            --timeout-seconds 900 \
            --query "Command.CommandId" --output text)

          echo "Sent SSM command: $CMD_ID"
          echo "::set-output name=command_id::$CMD_ID"

      - name: Wait for SSM command result
        env:
          INSTANCE_ID: ${{ steps.prep.outputs.instance_id }}
        run: |
          set -euo pipefail
          CMD_ID=${{ steps.send_ssm.outputs.command_id || steps.send_ssm.outputs.command_id }}
          INSTANCE_ID="${INSTANCE_ID}"
          echo "Polling SSM invocation status for command $CMD_ID on instance $INSTANCE_ID ..."
          ATTEMPTS=0
          while [ $ATTEMPTS -lt 120 ]; do
            sleep 5
            ((ATTEMPTS++))
            STATUS=$(aws ssm list-command-invocations --command-id "$CMD_ID" --details --instance-id "$INSTANCE_ID" --query "CommandInvocations[0].Status" --output text 2>/dev/null || echo "Unknown")
            echo "SSM invocation status: $STATUS"
            if [ "$STATUS" = "Success" ]; then
              echo "SSM command succeeded"
              exit 0
            fi
            if [[ "$STATUS" =~ ^(Failed|Cancelled|TimedOut|Undeliverable)$ ]]; then
              echo "SSM command finished with non-success status: $STATUS"
              aws ssm list-command-invocations --command-id "$CMD_ID" --details --instance-id "$INSTANCE_ID" || true
              exit 1
            fi
          done
          echo "Timeout waiting for SSM command"
          exit 1